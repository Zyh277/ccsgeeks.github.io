{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u56db\u5ddd\u5927\u5b66\u8ba1\u7b97\u673a\u7cfb\u7edf\u6280\u672f/\u79d1\u5b66\u7814\u7a76\u534f\u4f1a\u7814\u8ba8\u4f1a","text":""},{"location":"#_2","title":"\u7b80\u4ecb","text":""},{"location":"archive/","title":"Archives","text":""},{"location":"archive/240322/","title":"\u7814\u8ba8\u4f1a 03/22/2024","text":""},{"location":"archive/240322/#_1","title":"\u65f6\u95f4","text":"<p>2024 \u5e74 3 \u6708 22 \u65e5\u4e0b\u5348 15:30</p>"},{"location":"archive/240322/#_2","title":"\u5730\u70b9","text":"<p>\u6c5f\u5b89\u6821\u533a\u591a\u5b66\u79d1\u4ea4\u53c9\u521b\u65b0\u5927\u697c 1002 \u7814\u8ba8\u5ba4</p>"},{"location":"archive/240322/#_3","title":"\u4e3b\u9898\u5217\u8868","text":"\u6f14\u8bb2\u4eba \u6f14\u8bb2\u4e3b\u9898 \u9644\u4ef6 \u89c6\u9891 Tiger1218 RISC-V in a nutshell assets [links] junyu33 Verilog: A Practice Approach assets [links] t1d Linux Kernel Debugging [assets] [links] littfloret Windows SEH Mechanism [assets] [links]"},{"location":"materials/main/","title":"\u63a8\u8350\u7684\u4e66\u7c4d\u548c\u6750\u6599","text":""},{"location":"materials/main/#theory","title":"Theory","text":""},{"location":"materials/main/#analysis","title":"Analysis","text":"<p>Baby Rudin</p> <p>\u5e38\u5fae\u5206\u65b9\u7a0b \u738b\u9ad8\u96c4\u7b49</p> <p>Partial Differential Equations Lawrence C. Evans</p> <p>Analysis I Terence Tao</p> <p>\u51f8\u4f18\u5316\u6559\u7a0b Yuri Nesterov</p> <p>\u6570\u503c\u5206\u6790 Timothy Sauer \uff08\u4e25\u683c\u610f\u4e49\u4e0a Numerical Analysis \u4e0d\u80fd\u5f52\u4f5c\u5206\u6790\uff0c\u56e0\u4e3a\u91cc\u9762\u6709\u592a\u591a\u4ee3\u6570\u76f8\u5173\uff08\u6bd4\u5982\u6570\u503c\u77e9\u9635\u4ee3\u6570\uff09\u4e86\uff09</p> <p>\u4fe1\u53f7\u4e0e\u7cfb\u7edf Alan V. Oppenheim</p>"},{"location":"materials/main/#algebra","title":"Algebra","text":"<p>\u4ee3\u6570\u5b66\u8bb2\u4e49 \u674e\u6587\u5a01</p> <p>Linear Algebra Done Right Sheldon Jay Axler</p> <p>Algebra: Chapter 0 Paolo Aluffi</p> <p>\u77e9\u9635\u8ba1\u7b97 Gene H. Golub, Charles F. Van Loan</p>"},{"location":"materials/main/#_2","title":"\u6982\u7387\u8bba","text":"<p>A First Course in Probability, Sheldon M. Ross</p>"},{"location":"materials/main/#_3","title":"\u8ba1\u7b97\u673a\u76f8\u5173","text":"<p>Elements of Information Theory Thomas M. Cover</p> <p>Introduction to the Theory of Computation Michael Sipser</p> <p>Introduction to Graph Theory Douglas B. West</p> <p>\u8ba1\u7b97\u51e0\u4f55\uff1a\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u5206\u6790 \u9093\u4fca\u8f89</p>"},{"location":"materials/main/#_4","title":"\u5bc6\u7801\u5b66","text":"<p>An Introduction to Mathematical Cryptography Jeffrey Hoffstein, Jill Pipher, Joseph Silverman</p> <p>Introduction to Modern Cryptography Jonathan Katz and Yehuda Lindell</p>"},{"location":"materials/main/#computer-graphic","title":"Computer Graphic","text":"<p>Mathematics for 3D Game Programming and Computer Graphics Eric Lengyel</p> <p>Fundamentals of Computer Graphics Peter Shirley et al</p>"},{"location":"materials/main/#computer-systems","title":"Computer Systems","text":""},{"location":"materials/main/#overview","title":"Overview","text":"<p>Computer is a state machine: NJU PA</p> <p>\u53ef\u4ee5\u914d\u5408\u8881\u6625\u51e4\u8001\u5e08\u7684\u8ba1\u7b97\u673a\u7ec4\u6210\u4e0e\u7cfb\u7edf\u7ed3\u6784\u4e00\u8d77\u770b\u3002</p> <p>Application view of computer system: CSAPP</p>"},{"location":"materials/main/#architecture","title":"Architecture","text":"<p>Computer Architecture: A Quantitative Approach John L. Hennessy, David A. Patterson </p> <p>\u63a8\u8350\u8bed\uff1a\u795e\u4e66\u3002</p>"},{"location":"materials/main/#cpu-design","title":"CPU Design","text":"<p>CPU \u8bbe\u8ba1\u5b9e\u6218 \u6c6a\u6587\u7965</p> <p>RISC-V CPU \u5904\u7406\u5668\u8bbe\u8ba1/\u5de5\u7a0b\u4e0e\u5b9e\u8df5 \u80e1\u632f\u6ce2</p> <p>\u9ad8\u6027\u80fd\u8d85\u6807\u91cfCPU: \u5fae\u67b6\u6784\u5256\u6790\u4e0e\u8bbe\u8ba1 \u674e\u4e1c\u58f0\u3001\u4efb\u5b50\u6728\u3001\u5b59\u5c0f\u660e\u3001\u674e\u9e4f</p> <p>\u8d85\u6807\u91cf\u5904\u7406\u5668\u8bbe\u8ba1 \u59da\u6c38\u658c</p> <p>The Anatomy of a High Performance Microprocessor: A Systems Perspective Bruce Shriver</p> <p>\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406\uff08\u6e05\u534e\u5927\u5b66 2023 \u5e74\uff09</p> <p>\u4e00\u751f\u4e00\u82af\u9879\u76ee</p>"},{"location":"materials/main/#hardware-related","title":"Hardware Related","text":"<p>\u6570\u5b57\u8bbe\u8ba1: \u539f\u7406\u4e0e\u5b9e\u8df5</p> <p>CMOS \u8d85\u5927\u89c4\u6a21\u96c6\u6210\u7535\u8def\u8bbe\u8ba1</p>"},{"location":"materials/main/#_5","title":"\u8fdb\u9636","text":"<p>A Primer on Memory Consistency and Cache Coherence Daniel J. Sorin et al</p> <p>Memory Models Russ Cox</p> <p>on website</p>"},{"location":"materials/main/#isa","title":"\u5177\u4f53 ISA","text":"<p>The RISC-V Reader: An Open Architecture Atlas </p> <p>\u4e2d\u6587\u7248\u53ef\u4ee5\u5728 ysyx \u7684 \u7f51\u7ad9 \u4e0a\u9762\u83b7\u53d6\u5230\u3002</p>"},{"location":"materials/main/#operating-system","title":"Operating System","text":""},{"location":"materials/main/#linux-related","title":"Linux Related","text":"<p>\u9ed8\u8ba4\u4f60\u5df2\u5bf9 Linux Kernel Community \u5982\u4f55\u8fd0\u4f5c\u4e86\u5982\u6307\u638c\uff0c\u4e14\u4e0d\u9700\u8981\u6211\u4eec\u63d0\u9192\u4f60\u5982\u4f55\u770b\u5b98\u65b9\u6587\u6863/\u627e maillist \u5f52\u6863\u3002</p> <p>\u6df1\u5165 Linux \u5185\u6838\u67b6\u6784 </p> <p>Linux \u5185\u6838\u5b8c\u5168\u6ce8\u91ca v5.0</p> <p>Linux Inside</p> <p>Linux From Scrach</p> <p>Linux Kernel Development, Robert Love</p>"},{"location":"materials/main/#_6","title":"\u57fa\u7840\u6559\u6750","text":"<p>Operating Systems: Three Easy Pieces, Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau</p> <p>\u53ef\u4ee5\u5728 \u7f51\u9875 \u4e0a\u9762\u83b7\u53d6\u5728\u7ebf\u7248\u672c\u3002</p> <p>\u73b0\u4ee3\u64cd\u4f5c\u7cfb\u7edf\uff1a\u539f\u7406\u4e0e\u5b9e\u73b0, \u9648\u6d77\u6ce2\u3001\u590f\u865e\u658c</p>"},{"location":"materials/main/#_7","title":"\u8bfe\u7a0b\u4e3b\u9875","text":"<p>MIT 6.828</p> <p>...</p> <p>NJU OS</p> <p>\u6570\u5b66\u89c6\u89d2...</p>"},{"location":"materials/main/#high-performence-computing","title":"High Performence Computing","text":""},{"location":"materials/main/#theory-levels","title":"Theory Levels","text":"<p>The Art of Multiprocessor Programming, Maurice Herlihy, Nir Shavit</p> <p>Parallel and High Performance Computing, Robert Robey, Yuliana Zamora</p> <p>An Introduction to Parallel Programming, Peter S. Pacheco</p> <p>Structured Parallel Programming: Patterns for Efficient Computation, Michael McCool et al</p>"},{"location":"materials/main/#hardware-related_1","title":"Hardware Related","text":"<p>Intel\u00ae VTune\u2122 Profiler Performance Analysis Cookbook, Intel</p> <p>Available on Intel website</p> <p>Performance Analysis and Tuning on Modern CPUs, Denis Bakhvalov et al</p> <p>CUDA C++ Best Practices Guide, NVIDIA</p> <p>Available on NVIDIA website</p>"},{"location":"materials/main/#architecture-related","title":"Architecture Related","text":"<p>Processor Microarchitecture: An Implementation Perspective, Antonio Gonzalez et al</p>"},{"location":"materials/main/#application-levels","title":"Application Levels","text":"<p>\u9ad8\u6027\u80fd SQL, Baron Schwartz et al</p> <p>Designing Data-Intensive Applications, Martin Kleppmann</p> <p>\u6027\u80fd\u4e4b\u5dc5, Brendan Gregg</p> <p>BPF \u4e4b\u5dc5 Brendan Gregg</p> <p>OPEN MLSYS</p>"},{"location":"materials/main/#algorithm","title":"Algorithm","text":"<p>Concrete Mathematics: A Foundation for Computer Science, Ronald Graham, Donald Knuth, Oren Patashnik</p> <p>\u5b66\u8fc7\u7b97\u6cd5\u7ade\u8d5b\u7684\u90fd\u77e5\u9053\u3002</p> <p>Hacker's Delight, Henry S. Warren, Jr.</p> <p>\u7f16\u8bd1\u5668\u8bbe\u8ba1\u3001\u5d4c\u5165\u5f0f\u8bbe\u5907\u5f00\u53d1\u3001\u9ad8\u6027\u80fd\u6a21\u5757\u3001\u9006\u5411\u5de5\u7a0b\u7b49\u9886\u57df\u7684\u5de5\u5177\u4e66\u3002</p>"},{"location":"readings/llmsys/","title":"Overview of LLM Training","text":"<p>Main references:</p> <p>A Survey on Efficient Training of Transformers IJCAI'23</p> <p>Systems for Parallel and Distributed Large-Model Deep Learning Training</p> <p>* Efficient Training of Large Language Models on Distributed Infrastructures: A Survey</p> <p>( Latest )</p> <p></p>"},{"location":"readings/llmsys/#computation","title":"Computation","text":""},{"location":"readings/llmsys/#memory-efficiency","title":"Memory Efficiency","text":"<p>Analysis: </p> <p>Low-Memory Neural Network Training: A Technical Report</p>"},{"location":"readings/llmsys/#parallelism","title":"Parallelism","text":"<p>PyTorch Distributed Overview \u2014 PyTorch Tutorials 2.4.0+cu121 documentation</p> <p>PyTorch distributed system overview.</p> <p>Efficient Training on Multiple GPUs</p> <p>Huggingface's transformers multi-GPUs training guide.</p>"},{"location":"readings/llmsys/#data-parallelism","title":"Data Parallelism","text":""},{"location":"readings/llmsys/#simplest-data-parallel","title":"Simplest Data Parallel","text":"<p>DataParallel \u2014 PyTorch master documentation</p>"},{"location":"readings/llmsys/#parameter-sharing","title":"Parameter Sharing","text":"<p>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations ICLR'20</p>"},{"location":"readings/llmsys/#distributed-data-parallel","title":"Distributed Data Parallel","text":"<p>DistributedDataParallel \u2014 PyTorch master documentation</p> <p>PyTorch Distributed: Experiences on Accelerating Data Parallel Training</p> <p>Getting Started with Distributed Data Parallel \u2014 PyTorch Tutorials 2.4.0+cu121 documentation</p> <p>Distributed Data Parallel \u2014 PyTorch 2.4 documentation</p>"},{"location":"readings/llmsys/#fully-sharded-data-parallel","title":"Fully Sharded Data Parallel","text":"<p>PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel</p>"},{"location":"readings/llmsys/#zero","title":"ZeRO","text":"<p>ZeRO: Memory Optimizations Toward Training Trillion Parameter Models SC'20</p>"},{"location":"readings/llmsys/#zero-1","title":"ZeRO-1","text":""},{"location":"readings/llmsys/#zero-2","title":"ZeRO-2","text":""},{"location":"readings/llmsys/#zero-3","title":"ZeRO-3","text":"<p>Basic ZeRO.</p>"},{"location":"readings/llmsys/#model-parallelism","title":"Model Parallelism","text":"<p>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</p> <p>Two orthogonal main methods to boost model parallel: pipeline parallelism and tensor parallelism.</p>"},{"location":"readings/llmsys/#analysis","title":"Analysis","text":"<p>Analysis of model parallelism for distributed neural networks</p>"},{"location":"readings/llmsys/#pipeline-parallelism","title":"Pipeline Parallelism","text":"<p>* PipeDream: Fast and Efficient Pipeline Parallel DNN Training</p> <p>PipeDream: generalized pipeline parallelism for DNN training SOSP'19</p> <p>Pipelined Backpropagation at Scale: Training Large Models without Batches MLSys'21</p> <p>PipeMare: Asynchronous Pipeline Parallel DNN Training</p> <p>Memory-Efficient Pipeline-Parallel DNN Training ICML'21</p> <p>torchgpipe: On-the-fly Pipeline Parallelism for Training Giant Models</p> <p>Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines SC'21</p> <p>PipeMare: Asynchronous Pipeline Parallel DNN Training MLSys'21</p>"},{"location":"readings/llmsys/#pipeline-scheduler","title":"Pipeline Scheduler","text":"<p>* GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism NeurIPS'19</p> <p>DAPPLE: A Pipelined Data Parallel Approach for Training Large Models SOSP'21</p>"},{"location":"readings/llmsys/#tensor-parallelism","title":"Tensor Parallelism","text":"<p>Math: </p> <p>BLIS: A Framework for Rapidly Instantiating BLAS Functionality</p> <p>Framework:</p> <p>Mesh-TensorFlow: Deep Learning for Supercomputers NeurIPS'18</p> <p>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</p> <p></p>"},{"location":"readings/llmsys/#context-parallelism","title":"Context Parallelism","text":"<p>Reducing Activation Recomputation in Large Transformer Models MLSys'23</p> <p>Sequence Parallelism: Long Sequence Training from System Perspective</p>"},{"location":"readings/llmsys/#hybrid-parallel-3d-parallel","title":"Hybrid parallel / 3D Parallel","text":""},{"location":"readings/llmsys/#tuning-configuration","title":"Tuning Configuration","text":""},{"location":"readings/llmsys/#brute-force-approach","title":"Brute-force Approach","text":"<p>Parallelization Layouts for Large-Scale Distributed Model Training</p>"},{"location":"readings/llmsys/#theory-approach","title":"Theory Approach","text":"<p>* Beyond Data and Model Parallelism for Deep Neural Networks MLSys'19</p> <p>Supporting Very Large Models using Automatic Dataflow Graph Partitioning EuroSys'19</p> <p>* Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning OSDI'22</p> <p>Enabling Compute-Communication Overlap in Distributed Deep Learning Training Platforms ISCA'21</p>"},{"location":"readings/llmsys/#quantized","title":"Quantized","text":"<p>Mixed Precision Training ICLR'18</p>"},{"location":"readings/llmsys/#quantization","title":"Quantization","text":"<p>ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers NeurIPS'22</p> <p>Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks</p> <p>Training Deep Neural Networks with 8-bit Floating Point Numbers NeurIPS'18</p>"},{"location":"readings/llmsys/#activation-compressed-training","title":"Activation Compressed Training","text":"<p>ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training ICML'21</p>"},{"location":"readings/llmsys/#_1","title":"LLMSys","text":"<p>Mesa: A Memory-saving Training Framework for Transformers</p> <p>GACT: Activation Compressed Training for Generic Network Architectures ICML'22</p>"},{"location":"readings/llmsys/#checkpointing-rematerialization","title":"Checkpointing / Rematerialization","text":"<p>Originally purposed: </p> <p>Training Deep Nets with Sublinear Memory Cost</p> <p>Dynamic Tensor Rematerialization</p> <p>Optimal checkpointing for heterogeneous chains: how to train deep neural networks with limited memory</p> <p>Use in </p>"},{"location":"readings/llmsys/#compute-memory-balanced-checkpointing","title":"Compute-Memory Balanced Checkpointing","text":"<p>Accelerating the Training of Large Language Models using Efficient Activation Rematerialization and Optimal Hybrid Parallelism</p>"},{"location":"readings/llmsys/#offloading","title":"Offloading","text":""},{"location":"readings/llmsys/#swapping-problem","title":"Swapping Problem","text":"<p>* SwapAdvisor: Pushing Deep Learning Beyond the GPU Memory Limit via Smart Swapping ASPLOS'20</p> <p>Efficient Combination of Rematerialization and Offloading for Training DNNs NeurIPS'21</p> <p>ZeRO-Offload: Democratizing Billion-Scale Model Training ATC'21</p>"},{"location":"readings/llmsys/#to-disk","title":"To Disk","text":"<p>ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning SC'21</p>"},{"location":"readings/llmsys/#spilling","title":"Spilling","text":"<p>Training Large Neural Networks with Constant Memory using a New Execution Algorithm</p> <p>Hydra: A System for Large Multi-Model Deep Learning</p>"},{"location":"readings/llmsys/#pipeline-parallel-aware-offloading","title":"Pipeline-Parallel-Aware Offloading","text":"<p>Accelerating the Training of Large Language Models using Efficient Activation Rematerialization and Optimal Hybrid Parallelism</p>"},{"location":"readings/llmsys/#real-world-training-framework","title":"Real World Training Framework","text":"<p>Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training ICPP'23</p>"},{"location":"readings/llmsys/#deepspeed","title":"DeepSpeed","text":"<p>DeepSpeed Guide, Huggingface</p> <p>Main contribution: ZeRO (SC'20)</p>"},{"location":"readings/llmsys/#original-paper","title":"Original Paper","text":"<p>DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters SIGKDD'20</p> <p>Original paper.</p>"},{"location":"readings/llmsys/#_2","title":"LLMSys","text":""},{"location":"readings/llmsys/#zero_1","title":"ZeRO++","text":"<p>ZeRO++: Extremely Efficient Collective Communication for Giant Model Training</p>"},{"location":"readings/llmsys/#zero-infinity","title":"ZeRO-Infinity","text":"<p>ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning SC'21</p>"},{"location":"readings/llmsys/#moe","title":"MoE","text":"<p>DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale</p>"},{"location":"readings/llmsys/#megatron-lm","title":"Megatron-LM","text":"<p>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</p> <p>Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM SC'21</p>"},{"location":"readings/llmsys/#real-world-training-process","title":"Real World Training Process","text":"<p>Language Models are Few-Shot Learners</p> <p>Part B, D</p> <p>Accelerating the Training of Large Language Models using Efficient Activation Rematerialization and Optimal Hybrid Parallelism</p>"},{"location":"readings/llmsys/#megatron","title":"Megatron","text":"<p>microsoft/Megatron-DeepSpeed: Ongoing research training transformer language models at scale, including: BERT &amp; GPT-2</p> <p>Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model</p>"},{"location":"readings/llmsys/#cv","title":"CV","text":"<p>LAMP: Large Deep Nets with Automated Model Parallelism for Image Segmentation</p>"},{"location":"readings/llmsys/#inference","title":"Inference","text":"<p>GPU inference</p>"},{"location":"readings/llmsys/#automatic-mixed-precision","title":"Automatic Mixed Precision","text":"<p>Automatic Mixed Precision \u2014 PyTorch Tutorials 2.4.0+cu121 documentation</p>"},{"location":"readings/llmsys/#references","title":"References","text":"<p>\u6570\u636e\u5e76\u884cDeep-dive: \u4eceDP \u5230 Fully Sharded Data Parallel \uff08FSDP\uff09\u5b8c\u5168\u5206\u7247\u6570\u636e\u5e76\u884c - \u77e5\u4e4e</p> <p>DeepSpeed: Extreme-scale model training for everyone - Microsoft Research</p> <p>ZeRO &amp; DeepSpeed: New system optimizations enable training models with over 100 billion parameters - Microsoft Research</p> <p>\u5168\u7f51\u6700\u5168-\u8d85\u5927\u6a21\u578b+\u5206\u5e03\u5f0f\u8bad\u7ec3\u67b6\u6784\u548c\u7ecf\u5178\u8bba\u6587 - \u77e5\u4e4e</p> <p>11.2. \u5b9e\u73b0\u65b9\u6cd5 \u2014 \u673a\u5668\u5b66\u4e60\u7cfb\u7edf\uff1a\u8bbe\u8ba1\u548c\u5b9e\u73b0 1.0.0 documentation</p> <p>A Reading List for MLSys</p> <p>\u5927\u89c4\u6a21AI\u9ad8\u6027\u80fd\u7f51\u7edc\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u8df5_\u767e\u5ea6_\u9c81\u51ac\u96ea_InfoQ\u7cbe\u9009\u89c6\u9891</p>"},{"location":"upcoming/","title":"Upcomming seminars","text":""}]}